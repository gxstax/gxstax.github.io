---
title: k8s心得
comments: true
date: 2022-03-22 23:03:42
updated: 2022-04-03 23:03:42
desc: k8s心得
categories:
    - [云原生, k8s]
tags: 
    - k8s
keywords: k8s, cloudnative
---

## K8S心得

### pod

* K8s 为什么需要pod

  起因

  > 操作系统中，进程一般都是以进程组的方式，“有原则地”组织在一起。我们都知道容器的本质其实是进程，而我们的应用启动时其实就是这个PID=1的一个进程，而且我们的应用往往都存在着类似“进程和进程组”的关系，从而使他们保持着密切的协作关系，那么就使得我们必须把它们部署在同一台机器上。
  >
  > 我们先来看看，我们单独部署容器会有什么问题，假设我们现在的k8s有两个节点: node1（3GB）和node2(2.5GB),并且两个节点的资源分别是3GB和2GB，现在我们项目有3个容器 main、main-1、main-2，他们所需要的资源分别是 main(1GB)、main-1(1GB)、main-2(1GB);并且这3个容器是需要紧密合作的，必须部署在同一台机器上，假设我们用Docker Swarm来运行这个我们的项目，为了能够让这三个容器都运行在同一台机器上，我就必须在另外两个容器上设置一个 affinity=main（与 main 容器有亲密性）的约束，即：它们俩必须和 main 容器运行在同一台机器上。那么ABC分别进入Swarm待调度队列，然后main，main-1先后出队并被调度到了node2上(这个node2已经用掉了2GB的资源)。
  >
  > 当main-2容器出队开始被调度时，Swarm就有点懵了，因为此时的node2的资源只剩0.5GB，并不足以运行main-2容器；可是，根据 affinity=main 的约束，main-2 容器又只能运行在 node2上。

  一些解决方案的不足

  > 在工业界和学术界，关于这个问题的讨论可谓旷日持久，也产生了很多可供选择的解决方案。比如，Mesos 中就有一个资源囤积（resource hoarding）的机制，会在所有设置了 Affinity 约束的任务都达到时，才开始对它们统一进行调度。而在 Google Omega 论文中，则提出了使用乐观调度处理冲突的方法，即：先不管这些冲突，而是通过精心设计的回滚机制在出现了冲突之后解决问题。
  >
  > 可是这些方法都谈不上完美。资源囤积带来了不可避免的调度效率损失和死锁的可能性；而乐观调度的复杂程度，则不是常规技术团队所能驾驭的。

  解决方案

  > 到了 Kubernetes 项目里，这样的问题就迎刃而解了：Pod 是 Kubernetes 里的原子调度单位。这就意味着，Kubernetes 项目的调度器，是统一按照 Pod 而非容器的资源需求进行计算的。所以，像 main-1、main-2 和 main 函数主进程这样的三个容器，正是一个典型的由三个容器组成的 Pod。Kubernetes 项目在调度时，自然就会去选择可用内存等于 3 GB 的 node1 节点进行绑定，而根本不会考虑 node2。
